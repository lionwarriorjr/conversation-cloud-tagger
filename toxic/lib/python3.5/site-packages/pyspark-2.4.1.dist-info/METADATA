Metadata-Version: 2.0
Name: pyspark
Version: 2.4.1
Summary: Apache Spark Python API
Home-page: https://github.com/apache/spark/tree/master/python
Author: Spark Developers
Author-email: dev@spark.apache.org
License: http://www.apache.org/licenses/LICENSE-2.0
Platform: UNKNOWN
Classifier: Development Status :: 5 - Production/Stable
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 2.7
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.4
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: Implementation :: CPython
Classifier: Programming Language :: Python :: Implementation :: PyPy
Provides-Extra: ml
Provides-Extra: mllib
Provides-Extra: sql
Requires-Dist: py4j (==0.10.7)
Provides-Extra: ml
Requires-Dist: numpy (>=1.7); extra == 'ml'
Provides-Extra: mllib
Requires-Dist: numpy (>=1.7); extra == 'mllib'
Provides-Extra: sql
Requires-Dist: pandas (>=0.19.2); extra == 'sql'
Requires-Dist: pyarrow (>=0.8.0); extra == 'sql'

Apache Spark
============

Spark is a fast and general cluster computing system for Big Data. It
provides high-level APIs in Scala, Java, Python, and R, and an optimized
engine that supports general computation graphs for data analysis. It
also supports a rich set of higher-level tools including Spark SQL for
SQL and DataFrames, MLlib for machine learning, GraphX for graph
processing, and Spark Streaming for stream processing.

http://spark.apache.org/

Online Documentation
--------------------

You can find the latest Spark documentation, including a programming
guide, on the `project web
page <http://spark.apache.org/documentation.html>`__

Python Packaging
----------------

This README file only contains basic information related to pip
installed PySpark. This packaging is currently experimental and may
change in future versions (although we will do our best to keep
compatibility). Using PySpark requires the Spark JARs, and if you are
building this from source please see the builder instructions at
`“Building
Spark” <http://spark.apache.org/docs/latest/building-spark.html>`__.

The Python packaging for Spark is not intended to replace all of the
other use cases. This Python packaged version of Spark is suitable for
interacting with an existing cluster (be it Spark standalone, YARN, or
Mesos) - but does not contain the tools required to set up your own
standalone Spark cluster. You can download the full version of Spark
from the `Apache Spark downloads
page <http://spark.apache.org/downloads.html>`__.

**NOTE:** If you are using this with a Spark standalone cluster you must
ensure that the version (including minor version) matches or you may
experience odd errors.

Python Requirements
-------------------

At its core PySpark depends on Py4J (currently version 0.10.7), but some
additional sub-packages have their own extra requirements for some
features (including numpy, pandas, and pyarrow).


